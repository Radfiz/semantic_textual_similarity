{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a21b3a9-cbf5-4920-8303-43ff8d0b3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb5d248-bd51-4f01-9d7f-43222dd7a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Очистка текста и токенизация\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаляем пунктуацию\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "class SimpleWord2Vec:\n",
    "    def __init__(self, window_size=2):\n",
    "        self.window_size = window_size\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.co_occur = defaultdict(lambda: defaultdict(int))\n",
    "        self.vectors = {}\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Обучение на коллекции текстов\"\"\"\n",
    "        # Собираем статистику\n",
    "        for text in texts:\n",
    "            tokens = preprocess(text)\n",
    "            for i, word in enumerate(tokens):\n",
    "                self.word_counts[word] += 1\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(tokens), i + self.window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j != i:\n",
    "                        self.co_occur[word][tokens[j]] += 1\n",
    "        \n",
    "        # Создание векторов (PMI)\n",
    "        total_word_pairs = sum(sum(inner.values()) for inner in self.co_occur.values())\n",
    "        for word in self.co_occur:\n",
    "            vector = []\n",
    "            word_total = sum(self.co_occur[word].values())\n",
    "            for context_word in self.word_counts:\n",
    "                # Правильный расчет PMI\n",
    "                joint = self.co_occur[word].get(context_word, 0)\n",
    "                p_word = self.word_counts[word] / total_word_pairs\n",
    "                p_context = self.word_counts[context_word] / total_word_pairs\n",
    "                p_joint = joint / total_word_pairs\n",
    "                \n",
    "                if p_joint > 0:\n",
    "                    pmi = math.log(p_joint / (p_word * p_context))\n",
    "                else:\n",
    "                    pmi = 0\n",
    "                \n",
    "                vector.append(pmi)\n",
    "            self.vectors[word] = np.array(vector)\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self):\n",
    "        self.model = SimpleWord2Vec()\n",
    "        self.texts = []\n",
    "    \n",
    "    def index(self, documents):\n",
    "        \"\"\"Индексация документов\"\"\"\n",
    "        self.texts = documents\n",
    "        self.model.train(documents)\n",
    "    \n",
    "    def cosine_sim(self, vec_a, vec_b):\n",
    "        \"\"\"Косинусная мера между векторами\"\"\"\n",
    "        if np.linalg.norm(vec_a) == 0 or np.linalg.norm(vec_b) == 0:\n",
    "            return 0\n",
    "        return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "    \n",
    "    def search(self, query, threshold=0.6):\n",
    "        \"\"\"Поиск совпадений\"\"\"\n",
    "        results = []\n",
    "        query_tokens = preprocess(query)\n",
    "        \n",
    "        for doc_id, text in enumerate(self.texts):\n",
    "            tokens = preprocess(text)\n",
    "            if not tokens:\n",
    "                continue\n",
    "                \n",
    "            # Вектор документа (среднее векторов слов)\n",
    "            doc_vector = sum(self.model.vectors.get(word, np.zeros(len(self.model.word_counts))) \n",
    "                         for word in tokens) / len(tokens)\n",
    "            \n",
    "            for phrase in query_tokens:\n",
    "                phrase_vec = self.model.vectors.get(phrase, np.zeros(len(self.model.word_counts)))\n",
    "                similarity = self.cosine_sim(phrase_vec, doc_vector)\n",
    "                \n",
    "                if similarity > threshold:\n",
    "                    # Поиск точной позиции\n",
    "                    for i, word in enumerate(tokens):\n",
    "                        word_sim = self.cosine_sim(\n",
    "                            self.model.vectors.get(word, np.zeros(len(self.model.word_counts))),\n",
    "                            phrase_vec\n",
    "                        )\n",
    "                        if word_sim > threshold:\n",
    "                            start_pos = sum(len(tokens[j]) + 1 for j in range(i))\n",
    "                            end_pos = start_pos + len(word)\n",
    "                            results.append({\n",
    "                                \"text\": text,\n",
    "                                \"target\": phrase,\n",
    "                                \"start_pos\": start_pos,\n",
    "                                \"end_pos\": end_pos,\n",
    "                                \"score\": similarity,\n",
    "                                \"doc_id\": doc_id\n",
    "                            })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f928d66d-0da4-45d5-a2ba-bb730ab9c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/NeuroEmotions_data.csv')\n",
    "\n",
    "search_engine = SemanticSearch()\n",
    "search_engine.index(df['doc_text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347b111a-f438-48d1-b4c5-060fcfc38cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_engine.search(\"склейка\", threshold=0.3)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Документ {res['doc_id']}:\")\n",
    "    print(f\"Текст: {res['text']}\")\n",
    "    print(f\"Найдено: '{res['text'][res['start_pos']:res['end_pos']]}'\")\n",
    "    print(f\"Позиция: {res['start_pos']}-{res['end_pos']}\")\n",
    "    print(f\"Схожесть: {res['score']:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aabf4d-9eb1-484d-9303-5b529f736b7a",
   "metadata": {},
   "source": [
    "## Описание работы системы семантического поиска:\n",
    "\n",
    "**1. Препроцессинг текста**\n",
    "\n",
    "Система начинает с очистки входного текста: переводит все символы в нижний регистр и удаляет пунктуацию, оставляя только слова и пробелы. Затем текст разбивается на отдельные слова (токены). Например, фраза \"Мама мыла раму!\" преобразуется в список [\"мама\", \"мыла\", \"раму\"].\n",
    "\n",
    "**2. Обучение модели Word2Vec**\n",
    "\n",
    "Система создает семантические векторные представления слов по следующему алгоритму:\n",
    "\n",
    "Собирает статистику совместной встречаемости слов в пределах заданного окна (по умолчанию 2 слова слева и справа)\n",
    "\n",
    "Для каждого слова рассчитывает PMI (Pointwise Mutual Information) - меру семантической связи между словами\n",
    "\n",
    "Строит векторное пространство, где каждое слово представлено вектором его PMI-значений со всеми другими словами в словаре\n",
    "\n",
    "**3. Индексация документов**\n",
    "\n",
    "При добавлении документов система:\n",
    "\n",
    "Сохраняет оригинальные тексты\n",
    "\n",
    "Для каждого документа вычисляет усредненный вектор как среднее векторов всех слов в документе\n",
    "\n",
    "Создает поисковый индекс, позволяющий быстро находить документы по их векторным представлениям\n",
    "\n",
    "**4. Поиск и ранжирование**\n",
    "\n",
    "При обработке поискового запроса система:\n",
    "\n",
    "Разбивает запрос на слова и находит их векторные представления\n",
    "\n",
    "Вычисляет косинусную схожесть между вектором запроса и векторами документов\n",
    "\n",
    "Отбирает документы с показателем схожести выше заданного порога (по умолчанию 0.6)\n",
    "\n",
    "**5. Определение позиций**\n",
    "\n",
    "Для найденных документов система дополнительно:\n",
    "\n",
    "Ищет точные позиции слов запроса в тексте\n",
    "\n",
    "При отсутствии точных совпадений находит семантически близкие слова\n",
    "\n",
    "Возвращает позиции начала и конца найденных фрагментов\n",
    "\n",
    "**6. Особенности работы**\n",
    "\n",
    "Полностью на Python без внешних зависимостей (кроме numpy)\n",
    "\n",
    "Автоматически обрабатывает слова, отсутствующие в словаре\n",
    "\n",
    "Учитывает семантическую близость, а не только точные совпадения\n",
    "\n",
    "Позволяет настраивать чувствительность поиска через порог схожести"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
