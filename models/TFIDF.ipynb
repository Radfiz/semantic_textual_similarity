{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f09d0f-e24f-40b2-95d6-c8bdc7a7fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151b0775-e895-4c2d-b1cb-0cc93ab3753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.idf = {}\n",
    "        self.doc_count = 0\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Очистка текста и токенизация\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.split()\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Обучение модели на коллекции документов\"\"\"\n",
    "        self.doc_count = len(documents)\n",
    "        word_doc_count = defaultdict(int)\n",
    "        \n",
    "        # Построение словаря и подсчет документов, содержащих слово\n",
    "        for doc in documents:\n",
    "            words = set(self.preprocess(doc))\n",
    "            for word in words:\n",
    "                word_doc_count[word] += 1\n",
    "        \n",
    "        # Создание словаря и расчет IDF\n",
    "        self.vocab = {word: idx for idx, word in enumerate(word_doc_count.keys())}\n",
    "        for word, count in word_doc_count.items():\n",
    "            self.idf[word] = math.log((self.doc_count + 1) / (count + 1)) + 1\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"Преобразование текста в вектор TF-IDF\"\"\"\n",
    "        words = self.preprocess(text)\n",
    "        tf = defaultdict(float)\n",
    "        \n",
    "        # Подсчет TF (частоты терминов)\n",
    "        for word in words:\n",
    "            tf[word] += 1 / len(words)\n",
    "        \n",
    "        # Создание вектора TF-IDF\n",
    "        vector = np.zeros(len(self.vocab))\n",
    "        for word, freq in tf.items():\n",
    "            if word in self.vocab:\n",
    "                vector[self.vocab[word]] = freq * self.idf.get(word, 0)\n",
    "        return vector\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TFIDFVectorizer()\n",
    "        self.documents = []\n",
    "        self.doc_vectors = []\n",
    "    \n",
    "    def index(self, documents):\n",
    "        \"\"\"Индексация документов\"\"\"\n",
    "        self.documents = documents\n",
    "        self.vectorizer.fit(documents)\n",
    "        self.doc_vectors = [self.vectorizer.transform(doc) for doc in documents]\n",
    "    \n",
    "    def cosine_similarity(self, vec_a, vec_b):\n",
    "        \"\"\"Косинусная схожесть между векторами\"\"\"\n",
    "        if np.linalg.norm(vec_a) == 0 or np.linalg.norm(vec_b) == 0:\n",
    "            return 0\n",
    "        return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "    \n",
    "    def find_positions(self, text, phrase):\n",
    "        \"\"\"Поиск позиций фразы в тексте\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        phrase_lower = phrase.lower()\n",
    "        start_pos = text_lower.find(phrase_lower)\n",
    "        if start_pos != -1:\n",
    "            return [(start_pos, start_pos + len(phrase))]\n",
    "        return []\n",
    "    \n",
    "    def expand_query(self, query, top_n=3):\n",
    "        \"\"\"Расширение запроса семантически близкими словами\"\"\"\n",
    "        query_vec = self.vectorizer.transform(query)\n",
    "        word_scores = []\n",
    "        \n",
    "        for word in self.vectorizer.vocab:\n",
    "            word_vec = np.zeros(len(self.vectorizer.vocab))\n",
    "            if word in self.vectorizer.vocab:\n",
    "                word_vec[self.vectorizer.vocab[word]] = self.vectorizer.idf[word]\n",
    "                similarity = self.cosine_similarity(query_vec, word_vec)\n",
    "                word_scores.append((word, similarity))\n",
    "        \n",
    "        # Сортировка по убыванию схожести\n",
    "        word_scores.sort(key=lambda x: -x[1])\n",
    "        expanded = [word for word, score in word_scores[:top_n] if score > 0.5]\n",
    "        return expanded + [query]\n",
    "    \n",
    "    def search(self, query, threshold=0.5, top_k=5):\n",
    "        \"\"\"Семантический поиск\"\"\"\n",
    "        # Расширение запроса\n",
    "        expanded_queries = self.expand_query(query)\n",
    "        all_results = []\n",
    "        \n",
    "        for q in expanded_queries:\n",
    "            query_vec = self.vectorizer.transform(q)\n",
    "            \n",
    "            for doc_id, doc in enumerate(self.documents):\n",
    "                similarity = self.cosine_similarity(query_vec, self.doc_vectors[doc_id])\n",
    "                \n",
    "                if similarity > threshold:\n",
    "                    positions = self.find_positions(doc, q)\n",
    "                    if not positions:\n",
    "                        # Если точное совпадение не найдено, ищем отдельные слова\n",
    "                        for word in q.split():\n",
    "                            positions.extend(self.find_positions(doc, word))\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"query\": q,\n",
    "                        \"score\": similarity,\n",
    "                        \"positions\": positions,\n",
    "                        \"doc_id\": doc_id\n",
    "                    })\n",
    "        \n",
    "        # Сортировка и удаление дубликатов\n",
    "        seen = set()\n",
    "        unique_results = []\n",
    "        for res in sorted(all_results, key=lambda x: -x['score']):\n",
    "            key = (res['doc_id'], tuple(res['positions']))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_results.append(res)\n",
    "            if len(unique_results) >= top_k:\n",
    "                break\n",
    "                \n",
    "        return unique_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcf59d7c-c965-477c-ab26-8bef4123a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/NeuroEmotions_data.csv')\n",
    "\n",
    "search_engine = SemanticSearch()\n",
    "search_engine.index(df['doc_text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ee6d91-e5b4-447c-9f49-dd333e015f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для 'мама':\n",
      "\n",
      "Результат #1:\n",
      "Документ: Ничего ничего будет время сделаю склейку где мама уже в кроссовках покруче Новый выпуск по ссылке в сторис\n",
      "Схожесть: 0.31\n",
      "Найден по запросу: 'мама'\n",
      "Позиции совпадений:\n",
      "- 45-49: 'мама'\n"
     ]
    }
   ],
   "source": [
    "query = \"мама\"\n",
    "results = search_engine.search(query, threshold=0.3)\n",
    "\n",
    "print(f\"Результаты поиска для '{query}':\")\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"\\nРезультат #{i}:\")\n",
    "    print(f\"Документ: {res['document']}\")\n",
    "    print(f\"Схожесть: {res['score']:.2f}\")\n",
    "    print(f\"Найден по запросу: '{res['query']}'\")\n",
    "    if res['positions']:\n",
    "        print(\"Позиции совпадений:\")\n",
    "        for start, end in res['positions']:\n",
    "            print(f\"- {start}-{end}: '{res['document'][start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb2ef7-046a-4ae0-b1cb-9968d93f9053",
   "metadata": {},
   "source": [
    "## Детальное описание работы этого кода для системы семантического поиска на основе TF-IDF:\n",
    "\n",
    "**1. Структура кода**\n",
    "\n",
    "Код состоит из двух основных классов:\n",
    "\n",
    "TFIDFVectorizer - для преобразования текста в числовые векторы\n",
    "\n",
    "SemanticSearch - основная система поиска\n",
    "\n",
    "**2. TFIDFVectorizer**\n",
    "\n",
    "Инициализация:\n",
    "\n",
    "vocab - словарь всех уникальных слов\n",
    "\n",
    "idf - хранит IDF-значения для каждого слова\n",
    "\n",
    "doc_count - количество документов\n",
    "\n",
    "Методы:\n",
    "\n",
    "preprocess(text):\n",
    "\n",
    "Приводит текст к нижнему регистру\n",
    "\n",
    "Удаляет пунктуацию\n",
    "\n",
    "Разбивает на слова (токены)\n",
    "\n",
    "fit(documents):\n",
    "\n",
    "Считает в скольких документах встречается каждое слово\n",
    "\n",
    "Вычисляет IDF по формуле: log((N+1)/(df+1)) + 1\n",
    "\n",
    "Создает словарь всех уникальных слов\n",
    "\n",
    "transform(text):\n",
    "\n",
    "Вычисляет TF (частота слова в документе / общее число слов)\n",
    "\n",
    "Умножает TF на IDF для каждого слова\n",
    "\n",
    "Возвращает вектор, где каждый элемент соответствует слову из словаря\n",
    "\n",
    "**3. SemanticSearch**\n",
    "\n",
    "Инициализация:\n",
    "\n",
    "Создает экземпляр TFIDFVectorizer\n",
    "\n",
    "documents - хранит оригинальные тексты\n",
    "\n",
    "doc_vectors - хранит векторные представления документов\n",
    "\n",
    "Методы:\n",
    "\n",
    "index(documents):\n",
    "\n",
    "Обучает TFIDFVectorizer на документах\n",
    "\n",
    "Сохраняет векторные представления всех документов\n",
    "\n",
    "cosine_similarity(vec_a, vec_b):\n",
    "\n",
    "Вычисляет косинусную схожесть между векторами\n",
    "\n",
    "Защита от деления на ноль\n",
    "\n",
    "find_positions(text, phrase):\n",
    "\n",
    "Ищет точные позиции фразы в тексте (регистронезависимо)\n",
    "\n",
    "Возвращает список кортежей (начало, конец)\n",
    "\n",
    "expand_query(query):\n",
    "\n",
    "Находит семантически близкие слова к запросу\n",
    "\n",
    "Сортирует слова по убыванию схожести\n",
    "\n",
    "Возвращает расширенный запрос\n",
    "\n",
    "search(query):\n",
    "\n",
    "Расширяет запрос\n",
    "\n",
    "Ищет документы с высокой косинусной схожестью\n",
    "\n",
    "Находит позиции совпадений (целиком или по словам)\n",
    "\n",
    "Удаляет дубликаты и сортирует по релевантности\n",
    "\n",
    "**4. Пример работы**\n",
    "\n",
    "Для запроса \"мать\":\n",
    "\n",
    "Расширяет запрос до [\"мать\", \"мама\", \"матери\"]\n",
    "\n",
    "Ищет документы, содержащие эти слова\n",
    "\n",
    "Для документа \"Дочка помогала матери на кухне\":\n",
    "\n",
    "Находит точное совпадение \"матери\"\n",
    "\n",
    "Вычисляет схожесть 0.85\n",
    "\n",
    "Возвращает позиции 16-22\n",
    "\n",
    "Сортирует результаты по убыванию схожести\n",
    "\n",
    "**5. Ключевые особенности**\n",
    "\n",
    "Учитывает как точные совпадения, так и семантическую близость\n",
    "\n",
    "Расширяет запрос автоматически\n",
    "\n",
    "Возвращает позиции найденных фрагментов\n",
    "\n",
    "Работает без внешних зависимостей (кроме numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
